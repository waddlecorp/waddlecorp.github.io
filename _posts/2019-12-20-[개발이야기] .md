---
layout:     post
title:      "[개발이야기] 나랏말싸미 NLP를 만나다 - 임베딩 편"
subtitle:   " \"블루펭귄 개발 과정\""
date:       2019-12-16 12:00:00
author:     "예띠"
catalog: true
tags:
    - Python
    - pip
    - NLP
    - Text Preprocess
    - Data Science
    - 파이썬
    - 자연어처리
    - 한국어
    - 데이터 사이언스
    - 전처리
---

### 블루펭귄 개발 과정

주식회사 와들의 첫 iOS 앱인 **[블루펭귄](https://www.waddlelab.com/)**을 개발하는 과정을 담은 글이다.

#### 텍스트 전처리, 그 후
지난 포스트에서는 한국어 데이터셋의 텍스트 전처리를 다루었다. 이번 포스트에서는 컴퓨터가 자연어를 계산할 수 있도록 처리하는 방식인 임베딩 (embedding)에 대해서 알아보자.

#### 임베딩이란
임베딩은 자연어를 벡터로 변환하는 과정이나 그 결과를 말한다. 벡터로 변환된 자연어는 다양한 모델의 입력값으로 사용된다. 임베딩은 단어, 혹은 문장 수준에서 이루어진다. 이 포스트에서는 단어 수준의 임베딩에 사용되는 모델의 특성을 다루고, 다음 포스트에서 문장 수준의 임베딩에 사용되는 모델의 특성을 살펴볼 것이다.

#### 단어 수준의 임베딩은
단어 수준에서 이루어지는 임베딩은 문장 안의 단어를 따로따로 나누어 벡터로 만든다. 단어를 벡터로 변환하는 모델은 여러 가지가 있는데 Word2Vec, FastText, Glove 등이 있다.

#### Word2Vec
Word2Vec은 단어 간 유사도를 반영하여 단어의 의미를 벡터화한 모델이다. Word2Vec은 임베딩에 분산 표상 (distributed representation) 방법을 사용한다. 일반적으로 머신러닝에서 'representation'은 '표현'으로 번역되곤 하는데 다음의 페이스북 게시글을 참고하여 '표상'이라는 단어를 사용했다. 우리의 마음 속에 떠오르는 단어의 뜻을 벡터화한다는 점에서 표상이라는 말이 더 적절하다고 생각한다.

<iframe src="https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fpolytude%2Fposts%2F2863349267063037&width=500" width="500" height="569" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe>

분산 표상은 '문장에서 비슷한 위치에 있는 단어는 



[이 사이트](http://w.elnn.kr/search/)에 접속하면 한국어 단어들의 임베딩을 연산해볼 수 있다.

#### FastText


#### Glove


#### 앞으로
다음 시간에는 문장 수준에서 이루어지는, 최신 임베딩 기법들에 대해 살펴볼 것이다.