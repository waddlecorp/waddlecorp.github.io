---
layout:     post
title:      "[개발이야기] 나랏말싸미 NLP를 만나다 - 단어 임베딩 편 2"
subtitle:   " \"블루펭귄 개발 과정\""
date:       2020-01-06 12:00:00
author:     "예띠"
catalog: true
tags:
    - Python
    - pip
    - NLP
    - Text Preprocess
    - Data Science
    - 파이썬
    - 자연어처리
    - 한국어
    - 데이터 사이언스
    - 전처리
---

### 블루펭귄 개발 과정

주식회사 와들의 첫 iOS 앱인 **[블루펭귄](https://www.waddlelab.com/)**을 개발하는 과정을 담은 글이다.

#### 1편을 이으며
2편에서는 Glove를 다룬다.

#### GloVe란
**GloVe(Global Word Vectors)**는 **LSA(Latent Semantic Analysis)**와 **Word2Vec**의 단점을 극복하고자 개발된 단어 임베딩 기법이다. LSA는 각 문서에서의 각 단어의 빈도를 센 행려를 축소(truncated SVD)하여 의미를 이끌어내는 방법이며, Word2Vec은 주변 단어로 중심 단어를 예측하는 방법이다. LSA에 대한 자세한 설명은 [이곳](https://wikidocs.net/24949), Word2Vec은 [이곳](https://waddlecorp.github.io/2020/01/06/%EA%B0%9C%EB%B0%9C%EC%9D%B4%EC%95%BC%EA%B8%B0-%EB%82%98%EB%9E%8F%EB%A7%90%EC%8B%B8%EB%AF%B8-NLP%EB%A5%BC-%EB%A7%8C%EB%82%98%EB%8B%A4-%EC%9E%84%EB%B2%A0%EB%94%A9-%ED%8E%B8-1/)을 참고하면 좋을 것이다.

LSA는 말뭉치 전체의 통계 정보를 활용하지만 결과물에서 단어/문서 간의 유사도를 측정하기는 어렵다. 한편 Word2Vec는 단어 벡터 사이이의 유사도를 측정하는 데 좋지만 윈도우 크기 내에서만 학습이 이루어지기 때문에 말뭉치 전체에서의 등장 정보 (**co-occurence**)를 얻을 수 없다. GloVe는 이 둘의 약점을 보완하여 단어 벡터 간 유사도 측정이 수월하면서도 말뭉치 전체의 통계 정보를 잘 반영하기 위해 새로운 **목적함수**를 정의하였다.

##### 목적함수
GloVe의 목적함수는 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 한다. GloVe을 연구한 스탠포드 대학의 논문에서는 다음과 같은 표를 예시로 들어 목적함수를 설명한다.

<img class="shadow" width="300" src="/img/01-glove.jpg" alt="표가 있는 사진"/>

